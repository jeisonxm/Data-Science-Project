{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeisonxm/Data-Science-Project/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydXsyJEgI0QL"
      },
      "source": [
        "Importamos todas las librerias que necesitemos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Vo_3wdOI70d"
      },
      "source": [
        "# Sección nueva"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rv6pHfWhICFM"
      },
      "outputs": [],
      "source": [
        "from tkinter import *\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "import numpy\n",
        "import tensorflow\n",
        "import json\n",
        "import random\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P31tafBPIsqR",
        "outputId": "6d1441d6-82a9-40fe-d3da-3f43277e9667"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.3/107.3 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from tflearn) (1.15.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from tflearn) (7.1.2)\n",
            "Building wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=c3c73fc1b81f868e22a80f43a468e4f3f300e9dc397c92d3f5d96c74a3b7ee93\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ],
      "source": [
        "pip install tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_u6aDYbIwZ5",
        "outputId": "35a0a1f8-bbcb-4d5a-968f-91a29825a1e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ]
        }
      ],
      "source": [
        "import tflearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bEq3qXZIF-q",
        "outputId": "e9e5daa2-e10c-48d7-89d2-74b97e023c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "stemmer = LancasterStemmer()\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHggQXmaKKNy"
      },
      "source": [
        "Abrimos los datos del archivo json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZFt7j1VJJmWg",
        "outputId": "48a14d12-408a-4813-940b-c2d4d724a5d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-2adfa55d414f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'contenido.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marchivo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mdatos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchivo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'contenido.json'"
          ]
        }
      ],
      "source": [
        "with open('contenido.json',encoding='utf-8') as archivo:\n",
        "  datos = json.load(archivo)\n",
        "\n",
        "print(datos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbe-fy-7KNnn"
      },
      "outputs": [],
      "source": [
        "palabras = []\n",
        "tags = []\n",
        "auxX = []\n",
        "auxY = []\n",
        "\n",
        "for contenido in datos['contenido']:\n",
        "  for patrones in contenido['patrones']:\n",
        "    auxPalabra = nltk.word_tokenize(patrones) # Nltk nos permite trabajar con lenguaje natural\n",
        "    palabras.extend(auxPalabra)\n",
        "    auxX.append(auxPalabra)\n",
        "    auxY.append(contenido[\"tag\"])\n",
        "\n",
        "    if contenido[\"tag\"] not in tags:\n",
        "      tags.append(contenido[\"tag\"])\n",
        "\n",
        "print(palabras,auxX,auxY,sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNw_wdMjL_kI"
      },
      "outputs": [],
      "source": [
        "palabras = [stemmer.stem(w.lower()) for w in palabras if w!='?']  \n",
        "palabras = sorted(list(set(palabras)))\n",
        "tags = sorted(tags)\n",
        "print(palabras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-3D7fbYOSVO"
      },
      "source": [
        "Ajustamos las salidas de palabras en 1 y 0 para ayudar el entrenamiento de la ia, en pocas palabras nos ayuda a identificar el tag y ayudar a dar una respuesta dependienod de la palabra y relacion que encuentra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "r0pFepCXNCt5",
        "outputId": "cc2d9a51-4880-4154-8091-831f745242b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-7fc724ce06a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mentrenamiento\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msalida\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msalidaVacia\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocumento\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauxY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Enumerate nos permite obtener una palabra y su indice y se maneja con indice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tags' is not defined"
          ]
        }
      ],
      "source": [
        "entrenamiento = []\n",
        "salida = []\n",
        "salidaVacia = [0 for _ in range(len(tags))]\n",
        "\n",
        "for x, documento in enumerate(auxY): # Enumerate nos permite obtener una palabra y su indice y se maneja con indice\n",
        "  cubeta = []\n",
        "  auxPalabra = [stemmer.stem(w.lower()) for w in documento]\n",
        "  for w in palabras:\n",
        "    if w in auxPalabra:\n",
        "      cubeta.append(1)\n",
        "    else:\n",
        "        cubeta.append(0)\n",
        "  filaSalida = salidaVacia[:]\n",
        "  filaSalida[tags.index(auxY[x])]=1\n",
        "  entrenamiento.append(cubeta)\n",
        "  salida.append(filaSalida)\n",
        "print(entrenamiento, salida, sep='\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKlzY1UFN1Of"
      },
      "outputs": [],
      "source": [
        "# Cambiamos las listas normales por listas de numpy para manejar mejor sus dimensiones \n",
        "entrenamiento = numpy.array(entrenamiento) \n",
        "salida = numpy.array(salida)\n",
        "\n",
        "tensorflow.compat.v1.reset_default_graph()\n",
        "red = tflearn.input_data(shape=[None,len(entrenamiento[0])])\n",
        "red = tflearn.fully_connected(red,10) # Esto es creando 10 neuronas conectadas\n",
        "red = tflearn.fully_connected(red,10) # SE vuelve a repetir para seguir conectadas entre todas 10\n",
        "red = tflearn.fully_connected(red,len(salida[0]),activation='softmax') # Aqui con respecto a las probabilidades sacamos el output\n",
        "red = tflearn.regression(red)\n",
        "\n",
        "modelo = tflearn.DNN(red)\n",
        "modelo.fit(entrenamiento,salida,n_epoch=500, batch_size=10) \n",
        "# n_epoch= es ver cuantas veces mira la informacion \n",
        "# batch_size es la cantidad de palabras que se usan en patrones\n",
        "modelo.save(\"modelo.tflearn\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93U3fLcabMih"
      },
      "outputs": [],
      "source": [
        "salida[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R9aa9vEpSMLo",
        "outputId": "d46b25ce-046a-4ccc-e854-d4c951998e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot:  te estare esperando\n",
            "Bot:  te estare esperando\n",
            "Bot:  te estare esperando\n",
            "Bot:  cuidate\n"
          ]
        }
      ],
      "source": [
        "def mainBot():\n",
        "  while True:\n",
        "    entrada = input(\"Tu: \")\n",
        "    cubeta = [0 for _ in range(len(palabras))]\n",
        "    entradaProcesada = nltk.word_tokenize(entrada)\n",
        "    entradaProcesada = [stemmer.stem(palabra.lower()) for palabra in entrada]\n",
        "    for palabraIndividual in entradaProcesada:\n",
        "      for i, palabra in enumerate(palabras):\n",
        "        if palabra == palabraIndividual:\n",
        "          cubeta[i] = 1\n",
        "    resultados = modelo.predict([numpy.array(cubeta)])\n",
        "    resultadosIndices = numpy.argmax(resultados)\n",
        "    tag = tags[resultadosIndices]\n",
        "    for tagAux in datos['contenido']:\n",
        "      if tagAux['tag'] == tag:\n",
        "        respuesta = tagAux['respuestas']\n",
        "    print(\"Bot: \", random.choice(respuesta))\n",
        "mainBot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAtYwYC4U3C_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}